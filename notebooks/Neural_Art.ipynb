{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "print(torch.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bgr = cv2.imread(\"head.jpg\")\n",
    "\n",
    "image_rgb = cv2.cvtColor(image_bgr,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "h,w,c = image_rgb.shape\n",
    "print(h,w)\n",
    "input_dataset = torch.meshgrid(\n",
    "    torch.linspace(0,h,h),\n",
    "    torch.linspace(0,w,w)\n",
    ")\n",
    "\n",
    "input_dataset  = torch.stack(input_dataset,2).reshape(-1,2)\n",
    "target_dataset = torch.tensor(image_rgb).reshape(-1,3).float() / 255.0\n",
    "\n",
    "print(\"Input Shape\" ,list(input_dataset.shape ))\n",
    "print(\"Target Shape\",list(target_dataset.shape))\n",
    "\n",
    "\n",
    "\n",
    "sigmas = [\n",
    "    400,#face center\n",
    "    100,#nose tip\n",
    "    350,\n",
    "    300,\n",
    "    200,# left cheek\n",
    "    200,# right cheek\n",
    "#     300,\n",
    "#     300,\n",
    "#     300,\n",
    "#     300,\n",
    "    200,\n",
    "    300,\n",
    "    200,\n",
    "]\n",
    "\n",
    "means = [\n",
    "    [1015,952],#face center\n",
    "    [1152,952],#nose tip\n",
    "    [918,777],\n",
    "    [923,1136],\n",
    "    [1121,686], # left cheek\n",
    "    [1121,1193],#right cheek\n",
    "#     [1276,747],\n",
    "#     [1286,1126],\n",
    "#     [1501,854],\n",
    "#     [1481,1054],\n",
    "    [580,665],\n",
    "    [670,942],\n",
    "    [539,1213],\n",
    "]\n",
    "\n",
    "for mean,sigma in zip(means,sigmas):\n",
    "    y,x = mean\n",
    "    cv2.circle(image_rgb,(x,y),sigma,(255,0,0),5)\n",
    "\n",
    "plt.imshow(image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,means,sigmas,dim):\n",
    "        super().__init__()\n",
    "        num_points = len(means)\n",
    "        \n",
    "        self.means  = nn.Parameter(torch.tensor(means ).unsqueeze(0).float(),requires_grad=False)\n",
    "        self.sigmas = nn.Parameter(torch.tensor(sigmas).unsqueeze(0).float(),requires_grad=False)\n",
    "        self.fc = nn.Linear(num_points,dim,bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        # add a dimension so that x broadcasts with the means\n",
    "        x = x.unsqueeze(1) #[batch_size, 1 , input_dim]\n",
    "\n",
    "        # Calculate the distance squared from each sample to each mean\n",
    "        dist_squared = ((x - self.means)**2).sum(dim=2)  #[batch_size, embedding_dim]\n",
    "\n",
    "        # \n",
    "        x = torch.exp(-0.5* dist_squared /(self.sigmas**2 + 10e-4) )\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def export(self):\n",
    "        \n",
    "        means_list  = self.means.squeeze(0).tolist()\n",
    "        sigmas_list = self.sigmas.squeeze(0).tolist()\n",
    "        features_list = self.fc.weight.transpose(1,0).tolist()\n",
    "        print(self.fc.weight.shape)\n",
    "        \n",
    "        return means_list, sigmas_list, features_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxPositionEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,means,sigmas,dim):\n",
    "        super().__init__()\n",
    "        num_anchors = len(means)\n",
    "        \n",
    "        means    = torch.tensor(means ).float() #[num_anchors,2]\n",
    "        sigmas   = torch.tensor(sigmas).float() #[num_anchors]\n",
    "        features = torch.randn(num_anchors,dim) #[num_anchors,dim]\n",
    "        \n",
    "        self.means    = nn.Parameter(means   , requires_grad=True)\n",
    "        self.sigmas   = nn.Parameter(sigmas  , requires_grad=True)\n",
    "        self.features = nn.Parameter(features, requires_grad=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        # add a dimension so that x broadcasts with the means\n",
    "        x = x.unsqueeze(1) #[batch_size, 1 , input_dim]\n",
    "\n",
    "        # Calculate the difference between the inputs and the means\n",
    "        diff = x - self.means.unsqueeze(0) #[batch_size, num_anchors , 2]\n",
    "        \n",
    "        # Calculate the distance squared from each sample to each mean\n",
    "        dist = (diff**2).sum(dim=2).sqrt()  #[batch_size, num_anchors]\n",
    "\n",
    "        weighting = - (dist / self.sigmas.unsqueeze(0))**2  #[batch_size, num_anchors]\n",
    "        \n",
    "        weighting = torch.softmax(weighting,dim=1) #[batch_size, num_anchors]\n",
    "        \n",
    "        x = weighting @ self.features\n",
    "        return x\n",
    "    \n",
    "    def export(self):\n",
    "        \n",
    "        means_list  = self.means.squeeze(0).tolist()\n",
    "        sigmas_list = self.sigmas.squeeze(0).tolist()\n",
    "        features_list = self.fc.weight.transpose(1,0).tolist()\n",
    "        print(self.fc.weight.shape)\n",
    "        \n",
    "        return means_list, sigmas_list, features_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,dim):\n",
    "        super().__init__()\n",
    "        f = 256\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim,f),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(f,f//2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(f//2,f//4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(f//4,3),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "#         norm = x.norm(p=2, dim=1, keepdim=True)\n",
    "#         x = x / norm\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model,input_dataset,target_dataset):\n",
    "    cv2.namedWindow(\"img\",0)\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    epochs = 1000\n",
    "    data_size = input_dataset.shape[0]\n",
    "    batch_size = 2**15\n",
    "    batch_count = data_size // batch_size\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=epochs)\n",
    "\n",
    "    output_image = torch.zeros_like(target_dataset).to(device)\n",
    "    \n",
    "    for epoch_i in trange(epochs):\n",
    "        indicies = torch.randperm(data_size)\n",
    "        indicies = indicies[:(batch_count*batch_size)]\n",
    "        indicies = indicies.reshape(-1,batch_size)\n",
    "\n",
    "        for batch_indicies in indicies:\n",
    "            input_tensor = input_dataset[batch_indicies].to(device)\n",
    "            target_tensor = target_dataset[batch_indicies].to(device)\n",
    "            \n",
    "            output_tensor = model(input_tensor)\n",
    "            \n",
    "            output_image[batch_indicies] = output_tensor.detach()\n",
    "            \n",
    "            loss = criterion(output_tensor,target_tensor)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "            \n",
    "        img_rgb = (output_image.reshape(1892, 1824,3)*255).detach().cpu().numpy()\n",
    "        img_rgb = np.clip(img_rgb,0,255).astype(np.uint8)\n",
    "        img_bgr = cv2.cvtColor(img_rgb,cv2.COLOR_BGR2RGB)\n",
    "        for mean,sigma in zip(encoder.means.data.cpu(),encoder.sigmas.data.cpu()):\n",
    "            y,x = mean\n",
    "            y = int(y)\n",
    "            x = int(x)\n",
    "            sigma=int(sigma)\n",
    "            cv2.circle(img_bgr,(x,y),sigma,(255,0,0),5)\n",
    "        cv2.imshow(\"img\",img_bgr)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "encoder = SoftmaxPositionEncoder(means,sigmas,128)\n",
    "model = Model(128)\n",
    "seq = nn.Sequential(encoder,model)\n",
    "train(seq,input_dataset,target_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform():\n",
    "    def __init__(self,parent_transform = None,x=0.0,y=0.0,angle=0.0,scale=1.0):\n",
    "        \n",
    "        \n",
    "        if parent_transform is None:\n",
    "            self.mean_tensor_list    = []\n",
    "            self.std_tensor_list     = []\n",
    "            self.feature_tensor_list = []\n",
    "            \n",
    "            self.scale = 1.0\n",
    "            self.translation     = torch.tensor([[0.0,0.0]])\n",
    "            self.rotation_matrix = torch.tensor([[1.0,0.0],[0.0,1.0]])\n",
    "        else:\n",
    "            self.mean_tensor_list    = parent_transform.mean_tensor_list\n",
    "            self.std_tensor_list     = parent_transform.std_tensor_list\n",
    "            self.feature_tensor_list = parent_transform.feature_tensor_list\n",
    "            \n",
    "            translation = torch.tensor([[y,x]]).float()\n",
    "            c = math.cos(math.radians(angle))\n",
    "            s = math.sin(math.radians(angle)) \n",
    "            rotation_matrix = torch.tensor([[c,s],[-s,c]])\n",
    "            \n",
    "            self.translation = parent_transform.translation + translation @ parent_transform.rotation_matrix\n",
    "            self.rotation_matrix = parent_transform.rotation_matrix @ rotation_matrix * scale\n",
    "            self.scale = parent_transform.scale * scale\n",
    "            \n",
    "    def add_anchors(self,mean_tensor,std_tensor,feature_tensor):\n",
    "        mean_tensor = mean_tensor @ self.rotation_matrix\n",
    "        mean_tensor = mean_tensor + self.translation\n",
    "        std_tensor  = std_tensor  * self.scale\n",
    "        \n",
    "        self.mean_tensor_list.append(mean_tensor)\n",
    "        self.std_tensor_list.append(std_tensor)\n",
    "        self.feature_tensor_list.append(feature_tensor)\n",
    "        \n",
    "    def get_encoder_tensors(self):\n",
    "        mean_tensor    = torch.cat(self.mean_tensor_list   ,dim=0)\n",
    "        std_tensor     = torch.cat(self.std_tensor_list    ,dim=0)\n",
    "        feature_tensor = torch.cat(self.feature_tensor_list,dim=0)\n",
    "        \n",
    "        return mean_tensor,std_tensor,feature_tensor\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class Animator():\n",
    "    def __init__(self):\n",
    "        self.img_w = 1920\n",
    "        self.img_h = 1080\n",
    "        \n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.model = model\n",
    "        self.encoder = SoftmaxPositionEncoder([[0,0]],[500],64)\n",
    "        \n",
    "        self.encoder.to(self.device)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.pixel_coordinates = self.build_pixel_coordinates(self.img_w,self.img_h)\n",
    "        \n",
    "        \n",
    "    def render_animation(self,output_path,fps,length,draw_function):\n",
    "        \n",
    "        try:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "            video_writer = cv2.VideoWriter(output_path,fourcc, fps, (self.img_w, self.img_h))\n",
    "\n",
    "            for frame_i in trange(length):\n",
    "                transform = Transform()\n",
    "                draw_function(transform,frame_i)\n",
    "\n",
    "                mean_tensor,std_tensor,feature_tensor = transform.get_encoder_tensors()\n",
    "\n",
    "                self.encoder.means.data = mean_tensor\n",
    "                self.encoder.sigmas.data = std_tensor  \n",
    "                self.encoder.features.data = feature_tensor\n",
    "\n",
    "                self.encoder.to(self.device)\n",
    "\n",
    "                img_rgb = self.render_frame()\n",
    "                img_bgr = cv2.cvtColor(img_rgb,cv2.COLOR_BGR2RGB)\n",
    "                video_writer.write(img_bgr)\n",
    "                cv2.imshow(\"img\",img_bgr)\n",
    "                cv2.waitKey(1)\n",
    "        finally:\n",
    "            video_writer.release()\n",
    "            \n",
    "                \n",
    "    def build_pixel_coordinates(self,img_w,img_h):\n",
    "        \n",
    "        mesh_list = torch.meshgrid(\n",
    "            torch.arange(img_h),\n",
    "            torch.arange(img_w),\n",
    "        )\n",
    "        pixel_coordinates = torch.stack(mesh_list,2).reshape(-1,2)\n",
    "        \n",
    "        pixel_coordinates = pixel_coordinates.float().to(self.device)\n",
    "        \n",
    "        return pixel_coordinates\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def render_frame(self):\n",
    "        \n",
    "        # Pick a suitable batch size\n",
    "        batch_size  = 2**15\n",
    "        \n",
    "        # Get the number of pixel in the image\n",
    "        num_pixels  = len(self.pixel_coordinates)\n",
    "        \n",
    "        # Compute the number of batches\n",
    "        num_batches = int(np.ceil( num_pixels / batch_size ))\n",
    "        \n",
    "        # Create empty output tensor\n",
    "        output_tensor = torch.zeros(num_pixels,3)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # foreach batch\n",
    "            for i in range(num_batches):\n",
    "                \n",
    "                # Slice out a batch of coordinates\n",
    "                i1=i*batch_size\n",
    "                i2=min(i1+batch_size,num_pixels)\n",
    "                input_tensor = self.pixel_coordinates[i1:i2]\n",
    "                \n",
    "                # Run the coordinates through the encoder and model\n",
    "                output_tensor[i1:i2] = self.model(self.encoder(input_tensor))\n",
    "                \n",
    "        # Reshape output tensor back into image        \n",
    "        img_rgb = (output_tensor.reshape(self.img_h, self.img_w,3)*255).detach().cpu().numpy()\n",
    "        \n",
    "        # Conver to numpy image for opencv\n",
    "        img_rgb = np.clip(img_rgb,0,255).astype(np.uint8)\n",
    "\n",
    "        return img_rgb\n",
    "        \n",
    "       \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two heads rotating\n",
    "fps = 30\n",
    "run_time = 60\n",
    "total_frames = fps*run_time \n",
    "def draw(transform, frame_i):\n",
    "    \n",
    "    angle = frame_i/total_frames*2*360\n",
    "    t1 = Transform(transform,1920/2-500,1080/2,angle)\n",
    "    draw_face(t1)\n",
    "    t2 = Transform(transform,1920/2+500,1080/2,-angle)\n",
    "    draw_face(t2)\n",
    "    \n",
    "def draw_face(transform):\n",
    "    transform = Transform(transform,-962,-1024,0)\n",
    "    means=encoder.means.data.cpu()\n",
    "    sigmas=encoder.sigmas.data.cpu()\n",
    "    features=encoder.features.data.cpu()\n",
    "    transform.add_anchors(means,sigmas,features)\n",
    "    \n",
    "renderer = Animator()     \n",
    "renderer.render_animation(\"test.mp4\",fps,total_frames,draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two heads banging together\n",
    "fps = 30\n",
    "run_time = 60\n",
    "total_frames = fps*run_time \n",
    "def draw(transform, frame_i):\n",
    "    \n",
    "    angle = frame_i/total_frames*10*360\n",
    "    \n",
    "    s = math.sin(math.radians(angle-90))*5\n",
    "    s = 1 / (1+math.exp(-s))\n",
    "    s *= 500\n",
    "    \n",
    "    t1 = Transform(transform,1920/2-s,1080/2,0)\n",
    "    draw_face(t1)\n",
    "    t2 = Transform(transform,1920/2+s,1080/2,0)\n",
    "    draw_face(t2)\n",
    "    \n",
    "def draw_face(transform):\n",
    "    transform = Transform(transform,-962,-1024,0)\n",
    "    means=encoder.means.data.cpu()\n",
    "    sigmas=encoder.sigmas.data.cpu()\n",
    "    features=encoder.features.data.cpu()\n",
    "    transform.add_anchors(means,sigmas,features)\n",
    "    \n",
    "renderer = Animator()     \n",
    "renderer.render_animation(\"test.mp4\",fps,total_frames,draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two heads zoom together\n",
    "fps = 30\n",
    "run_time = 60\n",
    "total_frames = fps*run_time \n",
    "def draw(transform, frame_i):\n",
    "    \n",
    "    angle = frame_i/total_frames*20*360\n",
    "    \n",
    "    s = math.sin(math.radians(angle-90))*5\n",
    "    s = 1 / (1+math.exp(-s))\n",
    "    \n",
    "    s *= 0.8\n",
    "    s += 0.2\n",
    "    \n",
    "    t_center = Transform(transform,1920/2,1080/2,0)\n",
    "\n",
    "    t1 = Transform(t_center,x=-1920/3/2,scale=s)\n",
    "    draw_face(t1)\n",
    "    t2 = Transform(t_center,x=+1920/3/2,scale=s)\n",
    "    draw_face(t2)\n",
    "    \n",
    "def draw_face(transform):\n",
    "    transform = Transform(transform,-962,-1024,0)\n",
    "    means=encoder.means.data.cpu()\n",
    "    sigmas=encoder.sigmas.data.cpu()\n",
    "    features=encoder.features.data.cpu()\n",
    "    transform.add_anchors(means,sigmas,features)\n",
    "    \n",
    "renderer = Animator()     \n",
    "renderer.render_animation(\"test.mp4\",fps,total_frames,draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face Spin\n",
    "fps = 30\n",
    "run_time = 60\n",
    "total_frames = fps*run_time \n",
    "def draw(transform, frame_i):\n",
    "    \n",
    "    angle = frame_i/total_frames*2*360\n",
    "    \n",
    "    s = math.sin(math.radians(angle*5-90))*5\n",
    "    s = 1 / (1+math.exp(-s))\n",
    "    \n",
    "    s *= 0.99\n",
    "    s += 0.2\n",
    "    \n",
    "    t_center = Transform(transform,1920/2,1080/2,0)\n",
    "    t_spin = Transform(t_center,angle = angle,scale=0.8)\n",
    "    \n",
    "    spokes = 8\n",
    "    for i in range(spokes):\n",
    "        t1 = Transform(t_spin,angle=i/spokes*360)\n",
    "        t1 = Transform(t1,y=-1920/6*s,scale=.2,angle=60*s)\n",
    "        draw_face(t1)\n",
    "  \n",
    "    \n",
    "def draw_face(transform):\n",
    "    transform = Transform(transform,-962,-1024,0)\n",
    "    means=encoder.means.data.cpu()\n",
    "    sigmas=encoder.sigmas.data.cpu()\n",
    "    features=encoder.features.data.cpu()\n",
    "    transform.add_anchors(means,sigmas,features)\n",
    "    \n",
    "renderer = Animator()     \n",
    "renderer.render_animation(\"spin.mp4\",fps,total_frames,draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face grid split\n",
    "fps = 30\n",
    "run_time = 60\n",
    "total_frames = fps*run_time \n",
    "\n",
    "def map(start,end,ratio):\n",
    "    return start + (end-start)*ratio\n",
    "\n",
    "def draw(transform, frame_i):\n",
    "    \n",
    "    angle = frame_i/total_frames*360\n",
    "    pulse = frame_i/total_frames*4\n",
    "    \n",
    "    r = math.sin(math.radians(pulse*360-90))*5\n",
    "    r = 1 / (1+math.exp(-r))\n",
    "    \n",
    "    \n",
    "    \n",
    "    t_center = Transform(transform,1920/2,1080/2)   \n",
    "    w = 1920/5\n",
    "    h = 1080/3\n",
    "    z = map(.7,0.12,r)\n",
    "    for x in np.linspace(-2*w,2*w,5):\n",
    "        for y in np.linspace(-h,h,3):\n",
    "            x = map(0,x,r)\n",
    "            y = map(0,y,r)\n",
    "            t1 = Transform(t_center,x=x,y=y,scale=z,angle=angle)\n",
    "            draw_face(t1)\n",
    "  \n",
    "    \n",
    "def draw_face(transform):\n",
    "    transform = Transform(transform,-962,-920,0)\n",
    "    means=encoder.means.data.cpu()\n",
    "    sigmas=encoder.sigmas.data.cpu()\n",
    "    features=encoder.features.data.cpu()\n",
    "    transform.add_anchors(means,sigmas,features)\n",
    "    \n",
    "renderer = Animator()     \n",
    "renderer.render_animation(\"grid_pulse.mp4\",fps,total_frames,draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
